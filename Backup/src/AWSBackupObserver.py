# pylint: disable = C0103, R0902, W1203, C0301
"""
Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.

Permission is hereby granted, free of charge, to any person obtaining a copy of this
software and associated documentation files (the "Software"), to deal in the Software
without restriction, including without limitation the rights to use, copy, modify,
merge, publish, distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""
"""
This file contains the code for the Lambda function that handles Observer for 
AWS Backup solution. This code is also executed when an AWS Backup/Restore/Copy 
Event Notification is triggered from the completed Job.
"""
import json
import csv
import logging
import traceback
from datetime import timezone
import time
import os
import os.path
import datetime
from datetime import datetime
from email.mime.application import MIMEApplication
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from urllib.parse import urlparse
import boto3
import botocore
from botocore.exceptions import ClientError

#The max number of items EE can take
BACKUP_AUDIT_MANAGER_REPORT_MAX_ITEMS = 100
BACKUP_AUDIT_MANAGER_REPORT_ITEMS_KEY = 'reportItems'

######################################################################
#                      LOGGING HELPER FUNCTIONS                      #
######################################################################
logger = logging.getLogger()
logger.setLevel(logging.INFO)

def info(msg):
    """
    Helper function to log informational messages.
    """
    logger.info(json.dumps({'info': msg}))


def error(msg):
    """
    helper function to log error messages.
    """
    if isinstance(msg, ClientError):
        logger.error(json.dumps({'error': msg.response}))
    elif isinstance(msg, Exception):
        var = traceback.format_exc()
        logger.error(f'Error: {msg}' + json.dumps(f' Details : {var}'))
    else:
        logger.error(json.dumps(
            {'error': f'An unexpected error occurred: {repr(msg)}'}))


######################################################################
#                        CLASS DEFINITION                            #
######################################################################


class AWSBackupObserver:
    """
    AWSBackupObserver Controller class. Handles all resource creation/configuration
    operations and logic.
    """

    # INIT
    def __init__(self, event, context):

        self.session_id = boto3.session.Session().client('sts').get_caller_identity()
        self.account_id = boto3.client('sts').get_caller_identity()['Account']
        self.deployment_region = boto3.session.Session().region_name
        self.function_arn = context.invoked_function_arn

        # Extract the Uid from the ARN
        self.backup_manager_stack_id = os.environ.get('BackupManagerStackId').split('/')[2]

        # Names to identify the CloudWatch Events - Config Data and Report Generation
        self.config_data_refresh_schedule_name = os.environ.get('ConfigDataRefreshScheduleName')
        self.backup_report_schedule_name = os.environ.get('DailyJobReportSchedule')

        self.aws_config_supported_resource_types = os.environ.get('AWSConfigSupportedResourceTypes')
        if not self.aws_config_supported_resource_types:
            self.aws_config_supported_resource_types = \
                'AWS::EC2::Volume,AWS::EC2::Instance,AWS::RDS::DBInstance,AWS::DynamoDB::Table'

        # Report Configuration
        self.ses_email_subject = os.environ.get('SESEmailSubject')
        if not self.ses_email_subject:
            self.ses_email_subject = 'Backup Observer Solution for AWS Backup - Job Report (s) - {AccountId} - {LocalToday}'

        self.ses_email_body_text = os.environ.get('SESEmailBody')
        if not self.ses_email_body_text:
            self.ses_email_body_text = "<BR>\
         <BR> Please find your Report(s) attached in <b>CSV</b> format. \
         <BR> \
         <BR> {AttachmentDetails}\
         <BR> \
         <BR> Generated by the Backup Observer Solution for AWS Backup from : <b>{AccountId}</b> , at : {RegionId} for : <b>{UtcToday} </b>"

        self.lambda_temp_path = '/tmp' #nosec
        self.athena_query_expiration_timeout_in_milli_sec = 1000
        self.s3_athena_output_uri = os.environ.get('S3AthenaOutputURI')
        self.s3_email_report_output_bucket = os.environ.get('S3EmailReportOutputBucket')
        self.email_report_output_s3_key = os.environ.get('S3EmailReportOutputKey')
        self.global_event_bus_arn = os.environ.get('AWSBackupEventsBusArn')
        self.s3_observer_log_bucket = os.environ.get('S3BackupManagerLogBucket')
        self.s3_observer_log_location = os.environ.get('S3BackupManagerLogKey')

        self.report_on_all_dates = False
        if 'RefreshScope' in event and event['RefreshScope'] == 'All':
            self.report_on_all_dates = True

        self.ses_email_recipient_list = os.environ.get('SESEmailReportRecipientList')
        self.ses_email_sender = os.environ.get('SESEmailReportSender')
        self.s3_event_path_exclusions = os.environ.get('S3EventPathExclusions')


        #BackupAuditManager Entries
        self.backup_audit_manager_report_bucket = os.environ.get('BackupAuditManagerReportBucket')
        self.backup_audit_manager_report_folder = os.environ.get('BackupAuditManagerReportFolder')
        
        self.backup_audit_manager_reportitems_key = os.environ.get('BackupAuditManagerReportItemsKey')
        if not self.backup_audit_manager_reportitems_key:
            self.backup_audit_manager_reportitems_key = BACKUP_AUDIT_MANAGER_REPORT_ITEMS_KEY           
    def get_resource_id_from_arn(self, resource_arn):
        """
        Helper function to get the ResourceId from a given Arn.
        """
        # "ResourceArn":"arn:aws:ec2:{RegionId}:{AccountId}:volume/{VolumeId}"
        if -1 != resource_arn.find('/'):
            # Handle for EC2 and EBS
            resource_id = resource_arn.split("/")[1]
        # Assumed to like  "ResourceArn":"arn:aws:rds:{RegionId}:{AccountId}:db:{DBName}"
        elif -1 != resource_arn.find(':::'):
            # Handle for S3
            resource_id = resource_arn.split(":::")[1]   
        else:
            resource_id = resource_arn

        info(f"Resource Arn {resource_arn} produced id : {resource_id}")

        return resource_id

    def __get_tags_by_resource(self, resource_type, resource_id, resource_arn, recovery_point_arn):
        """
        Helper function to get Tags for a given resource detail
        """
        info(f"getTagsByResource resource_type {resource_type}, resourceId :{resource_id}," +
             f"resourceArn : {resource_arn}")

        resource_tag_list = {}
        recovery_point_tag_list = {}
        resource_type = resource_type.lower()
        backup_client = boto3.client('backup')

        if resource_type in ('elasticfilesystem', 'efs'):
            # Try getting tags from AWS Backup for supported resources
            # Get Tags for the recovery point
            # boto3 API /services/backup.html#Backup.Client.list_tags
            try:
    
                recovery_point_tag_list = backup_client.list_tags(ResourceArn=recovery_point_arn)
                if 'ResponseMetadata' in recovery_point_tag_list:
                    del recovery_point_tag_list['ResponseMetadata']
    
            except botocore.exceptions.ClientError as e:
                # Suppress as only EFS supported as of April 2021 -
                # AccessDeniedException (Not supported)
                # and InvalidParameterValueException (Missing recovery point)
                logger.error(f"{e.response['Error']}")

        if resource_type == 'dynamodb':
            dynamo = boto3.client('dynamodb')
            # bot3 API /services/dynamodb.html#DynamoDB.Client.list_tags_of_resource
            resource_tag_list = dynamo.list_tags_of_resource(ResourceArn=resource_arn)
            info(f'dynamodb resource_tag_list : {resource_tag_list}')
        elif resource_type in ('ec2', 'ebs'):

            info(f"getTagsByResource Type EC2 resourceId : {resource_id}")
            # Extract resource tag from the resource Arn
            # bot3 API services/ec2.html#EC2.Client.describe_tags
            ec2 = boto3.client('ec2')
            resource_tag_list = ec2.describe_tags(
                Filters=[
                    {
                        'Name': 'resource-id',
                        'Values': [
                            resource_id
                        ]
                    },
                ],
            )

            info(f'ec2 resource_tag_list : {resource_tag_list}')
        elif resource_type in ('rds', 'aurora'):
            rds = boto3.client('rds')
            # bot3 API services/rds.html#RDS.Client.list_tags_for_resource
            resource_tag_list = rds.list_tags_for_resource(ResourceName=resource_id)

            # Return is different from other resources. Unify the section
            resource_tag_list['Tags'] = resource_tag_list.pop('TagList')

        elif resource_type == 'fsx':
            fsx = boto3.client('fsx')
            # boto3 API  /services/fsx.html#FSx.Client.list_tags_for_resource
            resource_tag_list = fsx.list_tags_for_resource(ResourceARN=resource_arn)
            info(f'fsx resource_tag_list : {resource_tag_list}')
        elif resource_type == 'storagegateway':
            storagegateway = boto3.client('storagegateway')

            # boto3 API  /services/storagegateway.html#StorageGateway.Client.list_tags_for_resource
            resource_tag_list = storagegateway.list_tags_for_resource(ResourceARN=resource_arn)

        elif resource_type in ('elasticfilesystem', 'efs'):
            efs_client = boto3.client('efs')
            resource_tag_list = efs_client.list_tags_for_resource(ResourceId=resource_id)
        elif resource_type in ('s3'):
            s3_client = boto3.client('s3')
            resource_tag_list = s3_client.get_bucket_tagging(Bucket=resource_id)
            resource_tag_list['Tags'] = resource_tag_list.pop('TagSet') 
        elif resource_type in ('neptune'):
            neptune_client = boto3.client('neptune')
            resource_tag_list = neptune_client.list_tags_for_resource(ResourceName=resource_arn)
            # Return is different from other resources. Unify the section
            resource_tag_list['Tags'] = resource_tag_list.pop('TagList')
        elif resource_type in ('docdb','documentdb'):
            docdb_client = boto3.client('docdb')
            resource_tag_list = docdb_client.list_tags_for_resource(ResourceName=resource_arn)
            # Return is different from other resources. Unify the section
            resource_tag_list['Tags'] = resource_tag_list.pop('TagList')             
        else:
            info(f"{resource_type} Not Supported Yet, Fetchable from backup")
            return resource_tag_list

        if 'ResponseMetadata' in resource_tag_list:
            del resource_tag_list['ResponseMetadata']

        name_exist = False
        for tag_info in resource_tag_list['Tags']:
            if tag_info['Key'] == 'Name':
                name_exist = True

            if 'ResourceId' in tag_info:
                del tag_info['ResourceId']

            if 'ResourceType' in tag_info:
                del tag_info['ResourceType']

        info(f" name_exist in existing tags ? : {name_exist}")
        if not name_exist:
            # Handle RDS Naming arn:aws:rds:*:*:db:{DBName}
            resource_id = resource_id.split(':')[-1]
            # Handle instance/{InstanceId}
            resource_id = resource_id.split('/')[-1]
            info(f"Name Tag not Found.Adding Name Tag with ResourceId : {resource_id}")
            tag_info = {'Key': 'Name', 'Value': resource_id}
            resource_tag_list['Tags'].append(tag_info)

        try:
            if 'Tags' in recovery_point_tag_list:
                # Merge the tags - From resource and Recovery point
                for recovery_point_tag_key, recovery_point_tag_value in \
                        recovery_point_tag_list['Tags'].items():
                    info(
                        f"Appending Tags : {recovery_point_tag_key},{recovery_point_tag_value} to tags : {resource_tag_list['Tags']} ")
                    resource_tag_list['Tags'].append(
                        {'Key': recovery_point_tag_key, 'Value': recovery_point_tag_value})

        except Exception as e:
            logger.error(f"Error merging tags : {e}")
            var = traceback.format_exc()
            logger.error(f"Error {var} processing Merge the tags from resource and Recovery point")

        info(f"Resource Tag List : {resource_tag_list['Tags']}")
        return resource_tag_list

    def get_log_location(self, dest_folder, job_date, job_id):
        """
        Helper function to get Log location for the given job details
        """
        s3_log_location = self.s3_observer_log_location + '/' + dest_folder + '/'
        if job_date:
            # Add date to better index the data
            s3_log_location = s3_log_location + job_date + '/' + job_id
        elif job_id:
            s3_log_location = s3_log_location + job_id

        return s3_log_location

    def get_formatted_job_date(self, job_date):
        """
        Helper function to get the formatted job date
        """
        return str(job_date)[0:10]

    def save_event_data_to_s3(self, event_type, job_id, json_content):
        """
        This method is responsible for writing the job info (jsonContent) to the
        folder (destFolder) under the key (jobId)
        """
        if 'job_date' in json_content:
            s3_log_location = self.get_log_location(event_type,
                                                    json_content['job_date'], job_id)
        else:
            s3_log_location = self.get_log_location(event_type, '', job_id)

        if self.s3_observer_log_bucket:
            try:
                info(f"Persisting event data to : {self.s3_observer_log_bucket}")
                self.__write_object_to_s3(self.s3_observer_log_bucket, s3_log_location,
                                          json_content,
                                          'Observer')
            except (ClientError, Exception):
                var = traceback.format_exc()
                logger.error(f"Error {var} processing __write_object_to_s3")

    def send_event_to_observer_event_bus(self, observer_event_bus_arn, event_type, json_content):
        
        #arn:aws:events:us-east-1:301574290033:event-bus/GlobalBackupJobStatusEventBus-301574290033
        event_bus_dest_region = observer_event_bus_arn.split(':')[3]
        events_client = boto3.client('events',event_bus_dest_region)
        if observer_event_bus_arn:
            try:
                json_payload = json.dumps(json_content, default=str, separators=(',', ':'))
                info(
                    f"Sending event data to : {observer_event_bus_arn} under : {event_type}, json_content : {json_payload}")
                response = events_client.put_events(Entries=[
                    {
                        'Time': datetime.now(timezone.utc),
                        'Source': 'observer.command',
                        'DetailType': event_type,
                        'Detail': json_payload,
                        'EventBusName': observer_event_bus_arn
                    }
                ]
                )
                info(f"put_events response : {response}")
            except (ClientError, Exception):
                var = traceback.format_exc()
                logger.error(f"Error {var} processing put_events in send_event_to_observer_event_bus")
                
    def send_event_to_global_event_bus(self, event_type, json_content):

        events_client = boto3.client('events')
        if self.global_event_bus_arn:
            try:
                json_payload = json.dumps(json_content, default=str, separators=(',', ':'))
                info(
                    f"Sending event data to : {self.global_event_bus_arn} under : {event_type}, json_content : {json_payload}")
                response = events_client.put_events(Entries=[
                    {
                        'Time': datetime.now(timezone.utc),
                        'Source': 'observer.events',
                        'DetailType': event_type,
                        'Detail': json_payload,
                        'EventBusName': self.global_event_bus_arn
                    }
                ]
                )
                info(f"put_events response : {response}")
            except (ClientError, Exception):
                var = traceback.format_exc()
                logger.error(f"Error {var} processing put_events in send_event_to_global_event_bus")

    def __handle_event_data_persistence(self, event_type, job_id, json_content):
        """
        This method is responsible for writing the job info (jsonContent) to the
        folder (destFolder) under the key (jobId)
        """
        self.save_event_data_to_s3(event_type, job_id, json_content)
        self.send_event_to_global_event_bus(event_type, json_content)
        
    def enrich_copy_info_data(self,copy_info, additional_info_list):
        try:
            observer_event_bus_arn = ''
            copy_job_info = copy_info.get('CopyJob')
            info(f'Processing copy_job_info : {copy_job_info}')
            if copy_job_info:
                resource_type = copy_job_info['ResourceType'].lower()
                destination_recovery_point_arn = copy_job_info['DestinationRecoveryPointArn']            
                destination_backup_vault_arn = copy_job_info['DestinationBackupVaultArn']

                
                info(f'Processing resource_type : {resource_type} for destination_recovery_point_arn : {destination_recovery_point_arn}')
                if resource_type in ('ec2'):
                    resource_id = self.get_resource_id_from_arn(destination_recovery_point_arn) 
                    #arn:aws:backup:ap-southeast-1:497716000437:backup-vault:multiRegionCmkVault
                    copy_dest_region = destination_backup_vault_arn.split(':')[3]
                    copy_dest_account_id = destination_backup_vault_arn.split(':')[4]
                    copy_dest_partition = destination_backup_vault_arn.split(':')[1]
                    
                    info(f"Processing Type EC2 resourceId : {resource_id} at {copy_dest_region}, for {copy_dest_account_id}")
                    
                    if copy_dest_account_id != self.account_id:
                        #Send a command to destination observer to enrich the details
                        send_event_to_observer_event_bus = True
                        observer_event_bus_arn = f'arn:{copy_dest_partition}:events:{copy_dest_region}:{copy_dest_account_id}:event-bus/BoS-ObserverInwardJobEventBus-{copy_dest_account_id}'
                    else:
                        info(f"describe_images for  : {resource_id} at {copy_dest_account_id}")
                        # Extract backing snapshots from AMI Id
                        # bot3 API services/ec2.html#EC2.Client.describe_images
                        ec2 = boto3.client('ec2',copy_dest_region) 
                        describe_images_info = ec2.describe_images(ImageIds=[resource_id])
                        info(f"describe_images_info : {describe_images_info}")                    
                        if 'ResponseMetadata' in describe_images_info:
                            del describe_images_info['ResponseMetadata']
                         
                        for image in describe_images_info['Images']:
                            for block_dev_mapping in image['BlockDeviceMappings']:
                                snapshot_id = block_dev_mapping['Ebs']['SnapshotId']
                                account_id = copy_job_info['AccountId']
                                snapshot_arn = (f'arn:{copy_dest_partition}:ec2:{copy_dest_region}:{copy_dest_account_id}:snapshot/{snapshot_id}')
                                info(f'Adding snapshot_arn : {snapshot_arn} to additional_info_list')
                                additional_info_list.append(snapshot_arn)
                else:
                    info(f'Adding recovery_point_arn : {destination_recovery_point_arn} to additional_info_list')
                    additional_info_list.append(destination_recovery_point_arn)
        except Exception as e:
            logger.error(f"Error : {e} processing additional_info_list") 
            
        return observer_event_bus_arn
        
    def write_copy_event_data(self, job_id, job_event_type):
        """
        Helper function to write the Copy Job Event data
        """
        info(f"Processing writeCopyEventData with jobId : {job_id}")
        backup_client = boto3.client('backup')
        # boto3 API  /services/backup.html#Backup.Client.describe_copy_job
        copy_info = backup_client.describe_copy_job(CopyJobId=job_id)

        if 'ResponseMetadata' in copy_info:
            del copy_info['ResponseMetadata']


        tag_list = []
        if copy_info:
            recovery_point_arn= copy_info['CopyJob']['SourceRecoveryPointArn']
            resource_arn = copy_info['CopyJob']['ResourceArn']  
            resource_type = copy_info['CopyJob']['ResourceType']
            try:
                resource_id = self.get_resource_id_from_arn(resource_arn)
                tag_list = self.__get_tags_by_resource(resource_type, resource_id, resource_arn,
                                                       recovery_point_arn)
    
            except Exception as e:
                logger.error(f"Error : {e} processing tags")    
                
        additional_info_list = []
        observer_event_bus_arn = self.enrich_copy_info_data(copy_info, additional_info_list)
        
        info(f"enrich_copy_info_data additional_info_list : {additional_info_list}")    
        try:
            info(f"copy_info {copy_info}")
            # Log the Combined Event
            copy_event_data = {'job_id': job_id, 'job_region': self.deployment_region,
                               'job_type': job_event_type,
                               'job_date': self.get_formatted_job_date(
                                   copy_info['CopyJob']['CreationDate']),
                               'copy_info': copy_info,
                               'additional_info' : additional_info_list,
                               'tag_info': tag_list
            }
            self.__handle_event_data_persistence(job_event_type, job_id, copy_event_data)
            if observer_event_bus_arn:
                #arn:aws:events:us-east-1:301574290033:event-bus/GlobalBackupJobStatusEventBus-301574290033
                self.send_event_to_observer_event_bus(observer_event_bus_arn, job_event_type, copy_event_data)

        except (ClientError, Exception):
            var = traceback.format_exc()
            logger.error(f"Error {var} processing __handle_event_data_persistence")

    def write_restore_event_data(self, job_id, job_event_type):
        """
        Helper function to write the Restore Job Event data
        """
        info(f"Processing writeRestoreEventData with jobId : {job_id}")

        backup_client = boto3.client('backup')
        # boto3 API /services/backup.html#Backup.Client.describe_restore_job
        restore_info = backup_client.describe_restore_job(RestoreJobId=job_id)

        if 'ResponseMetadata' in restore_info:
            del restore_info['ResponseMetadata']

        backup_client = boto3.client('backup')
        # https://docs.aws.amazon.com/aws-backup/latest/devguide/aws-backup-limits.html
        # boto3 API  /services/backup.html#Backup.Client.list_backup_vaults
        backupvault_list = backup_client.list_backup_vaults()

        recovery_info = {}
        for backupVaultInfo in backupvault_list['BackupVaultList']:
            backup_vault_name = backupVaultInfo['BackupVaultName']
            recovery_point_arn = restore_info['RecoveryPointArn']
            try:
                # Figure out the backup details {An Adhoc Way, but should work}
                # boto3 API  /services/backup.html#Backup.Client.describe_recovery_point
                recovery_info = backup_client.describe_recovery_point(
                    BackupVaultName=backup_vault_name,
                    RecoveryPointArn=recovery_point_arn
                )

                if 'ResponseMetadata' in recovery_info:
                    del recovery_info['ResponseMetadata']
                # Break since information is already found
                break
            except botocore.exceptions.ClientError as e:
                if e.response['Error']['Code'] == "ResourceNotFoundException":
                    info(
                        f"Recovery Point ARN : {recovery_point_arn} not found in Vault : {backup_vault_name}")
                else:
                    error(e)
        
        tag_list = []
        if recovery_info:
            resource_arn = recovery_info['ResourceArn']  
            resource_type = recovery_info['ResourceType']
            try:
                resource_id = self.get_resource_id_from_arn(resource_arn)
                tag_list = self.__get_tags_by_resource(resource_type, resource_id, resource_arn,
                                                       recovery_point_arn)
    
            except Exception as e:
                logger.error(f"Error : {e} processing __get_tags_by_resource")                    

        try:
            # Log the Combined Event
            restore_event_data = {'job_id': job_id, 'job_region': self.deployment_region,
                                  'job_type': job_event_type,
                                  'job_date': self.get_formatted_job_date(
                                      restore_info['CreationDate']),
                                  'restore_info': restore_info, 
                                  'recovery_info': recovery_info,
                                  'tag_info': tag_list
                                 }
            self.__handle_event_data_persistence(job_event_type, job_id, restore_event_data)

        except Exception as e:
            logger.error(f"Error : {e} processing __handle_event_data_persistence")

    def write_backup_event_data(self, job_id, job_event_type):
        """
        Helper function to write the Backup Job Event data
        """
        backup_info = None
        resource_info = []
        restore_metadata = []
        tag_list = []
        backup_plan_info = []
        backup_client = boto3.client('backup')

        try:
            # boto3 API  /services/backup.html#Backup.Client.describe_backup_job
            backup_info = backup_client.describe_backup_job(BackupJobId=job_id)
            if 'ResponseMetadata' in backup_info:
                del backup_info['ResponseMetadata']

            info(f"backup_info : {backup_info}")
        except botocore.exceptions.ClientError as e:
                if e.response['Error']['Code'] == "ResourceNotFoundException":
                    logger.error(f"Backup Job with ID : {job_id} not found")
                else:
                    logger.error(f"Error : {e} processing describe_backup_job")


        if backup_info:
            backup_job_state = backup_info['State']
            backup_vault_name = backup_info['BackupVaultName']
            info(f"Job State is : {backup_job_state} for Vault : {backup_vault_name}")
    
            try:
    
                if 'CreatedBy' in backup_info:
                    backup_plan_id = backup_info['CreatedBy']['BackupPlanId']
                    backup_plan_version = backup_info['CreatedBy']['BackupPlanVersion']
    
                    info(
                        f"get_backup_plan for {backup_plan_id} and version {backup_plan_version}")
                    # boto3 API  /services/backup.html#Backup.Client.get_backup_plan
                    backup_plan_info = backup_client.get_backup_plan(BackupPlanId=backup_plan_id,
                                                                     VersionId=backup_plan_version)
    
                    info(
                        f"get_backup_plan for {backup_plan_id} and version {backup_plan_version} is {backup_plan_info}")
                    if 'ResponseMetadata' in backup_plan_info:
                        del backup_plan_info['ResponseMetadata']
    
            except Exception as e:
                logger.error(f"Error : {e} processing get_backup_plan")
    
            resource_arn = backup_info['ResourceArn']
            resource_type = backup_info['ResourceType']
            recovery_point_arn = ''
            if backup_job_state == 'COMPLETED':
                try:
                    recovery_point_arn = backup_info['RecoveryPointArn']
                    # boto3 API  /services/backup.html#Backup.Client.describe_protected_resource
                    resource_info = backup_client.describe_protected_resource(ResourceArn=resource_arn)
    
                    if 'ResponseMetadata' in resource_info:
                        del resource_info['ResponseMetadata']
    
                except Exception as e:
                    logger.error(f"Error : {e} processing describe_protected_resource")
    
                if recovery_point_arn:
                    try:
                        # boto3 API  /services/backup.html#Backup.Client.get_recovery_point_restore_metadata
                        restore_metadata = backup_client.get_recovery_point_restore_metadata(
                            BackupVaultName=backup_vault_name,
                            RecoveryPointArn=recovery_point_arn
                        )
    
                        if 'ResponseMetadata' in restore_metadata:
                            del restore_metadata['ResponseMetadata']
                    except Exception as e:
                        logger.error(f"Error : {e} processing get_recovery_point_restore_metadata")
    
            try:
                resource_id = self.get_resource_id_from_arn(resource_arn)
                tag_list = self.__get_tags_by_resource(resource_type, resource_id, resource_arn,
                                                       recovery_point_arn)
    
            except Exception as e:
                logger.error(f"Error : {e} processing __get_tags_by_resource")

            try:
                # Log the Combined Event
                backup_event_data = {'job_id': job_id, 'job_region': self.deployment_region,
                                     'job_type': job_event_type,
                                     'job_date': self.get_formatted_job_date(
                                         backup_info['CreationDate']),
                                     'backup_info': backup_info,
                                     'resource_info': resource_info,
                                     'restore_metadata': restore_metadata, 'tag_info': tag_list,
                                     'backup_plan_info': backup_plan_info}
                self.__handle_event_data_persistence(job_event_type, job_id, backup_event_data)
    
            except Exception as e:
                logger.error(f"Error : {e} processing __handle_event_data_persistence")
        else:
            logger.error(f"Backup Job with ID : {job_id} not found. Skipping processing")
            

    def job_details_exist_in_s3(self, job_event_type, job_date, job_id):
        """
        Helper function to check if the job details already exist at the S3 location.
        """
        object_exist = True
        s3_log_location = self.get_log_location(job_event_type, job_date, job_id)
        s3_client = boto3.client('s3')
        try:
            s3_client.head_object(Bucket=self.s3_observer_log_bucket, Key=s3_log_location)
        except ClientError as e:
            object_exist = False
            if e.response['Error']['Code'] == "404":
                # The object does not exist.
                info(
                    f"Object : {job_id} doesn't exist in S3 Bucket {self.s3_observer_log_bucket} at {s3_log_location}.")
            elif e.response['Error']['Code'] == "403":
                # Forbidden Error
                info(
                    f"AccessDeniedException. Check permissions for the bucket : {self.s3_observer_log_bucket}")
            else:
                # Something else has gone wrong.
                raise
        else:
            info(f"Object : {job_id} exists")

        return object_exist

    def move_s3_file(self, src_bucket, src_key, dest_bucket, dest_key):
        """
        Helper function to move athena results file to the selected S3 URI
        """
        try:
            s3_resource = boto3.resource('s3')
            info(
                f"srcBucket : {src_bucket}, srcKey : {src_key}, destBucket : {dest_bucket}, destKey : {dest_key} ")
            # Source Dictionary That Specifies Bucket Name and Key Name of the Object to Be Copied
            copy_source = {
                'Bucket': src_bucket,
                'Key': src_key
            }

            # Copying the Object to the Target Directory
            s3_resource.Bucket(dest_bucket).copy(copy_source, dest_key)
            s3_resource.ObjectAcl(dest_bucket, dest_key).put(ACL='bucket-owner-full-control')

            # To Delete the File After Copying It to the Target Directory
            s3_resource.Object(src_bucket, src_key).delete()
        except Exception as exc:
            error(
                f"Failed to move {src_key} to {dest_key} in Bucket : {src_bucket}, Exception: {str(exc)}")

    def move_older_files_to_archive(self, src_folder, dest_folder):
        """
          Helper function to move stale config files to archive location
        """
        s3_log_location = self.get_log_location(src_folder, '', '')
        datetime_today = datetime.now(timezone.utc)
        files_moved_to_archive = False
        s3_client = boto3.client('s3')
        response = s3_client.list_objects_v2(Bucket=self.s3_observer_log_bucket,
                                             Delimiter='/',
                                             Prefix=s3_log_location)

        if 'Contents' in response:
            for key_info in response['Contents']:
                date_diff = (datetime_today - key_info['LastModified'])
                days = date_diff.days
                # seconds = date_diff.seconds
                # hours = seconds // 3600
                # minutes = (seconds // 60) % 60
                if days > 1:
                    src_s3_key = key_info['Key']
                    dst_s3_key = key_info['Key'].replace('latest', 'archive')
                    info(f'Moving {src_s3_key} to {dst_s3_key}')

                    self.move_s3_file(self.s3_observer_log_bucket, src_s3_key,
                                      self.s3_observer_log_bucket,
                                      dst_s3_key)
                    files_moved_to_archive = True
            info(f'movedOlderFilesToArchive from {src_folder} to {dest_folder} at {datetime_today}')

        else:
            info(
                f'move_older_files_to_archive NOT executed for {src_folder} and {dest_folder}')
        return files_moved_to_archive

    def extract_config_details(self, ):
        """
        Helper function to extract config details using AWS Config query function
        and output to a predefined S3 location for query using AWS Athena.
        """

        config_select_query = "SELECT *,tags WHERE resourceType in ( "
        for config_item in self.aws_config_supported_resource_types.split(','):
            config_select_query = config_select_query + "'" + config_item + "',"

        config_select_query = config_select_query[:-1] + ")"
        info(f"config_select_query : {config_select_query}")
        config_client = boto3.client('config')
        # boto3 API  /services/config.html#ConfigService.Client.select_resource_config
        response = config_client.select_resource_config(Expression=config_select_query)
        # print(response)

        result_obj = {}
        # Create a list of newly created objects
        for result in response['Results']:
            result_obj = json.loads(result)
            #info(f"processing result_obj : {result_obj}")
            # find the unique arn for the object to spit out the config details
            flattened_arn = ''
            if 'arn' in result_obj:
                flattened_arn = result_obj['arn']
            elif 'resourceId' in result_obj:
                flattened_arn = result_obj['resourceId']

            flattened_arn = flattened_arn.replace(':', '_').replace('/', '_')
            result_obj['job_type'] = 'config_job/latest'
            result_obj['job_id'] = flattened_arn
            result_obj['job_region'] = self.deployment_region

            # Update the S3 bucket with the latest config data
            self.__handle_event_data_persistence('config_job/latest', flattened_arn, result_obj)

        # Clear the stale config files
        if self.move_older_files_to_archive('config_job/latest', 'config_job/archive'):
            result_obj['job_type'] = 'config_archive'
            # Send a notification to handle stale data
            self.send_event_to_global_event_bus('config_archive', result_obj)

    def query_athena_database(self, query_list):
        """
        Query Backup using Athena
        Run query string one by one, and storage query id as new key queryId in the query_list
        """
        client = boto3.client('athena')
        info("Starting query Backup ... ")
        for i in range(len(query_list)):
            info(f"Starting query : {query_list[i]['QueryString']}")
            resp = client.start_query_execution(QueryString=query_list[i]['QueryString'],
                                                ResultConfiguration={
                                                    'OutputLocation': self.s3_athena_output_uri})
            query_list[i]['queryId'] = resp['QueryExecutionId']
            info("Query " + query_list[i]['Name'] + ' , queryId is ' + query_list[i][
                'queryId'])

    def replace_variables(self, query_string):
        """
        Helper function to replace query variables using configuration items
        """
        for env_key, env_value in sorted(os.environ.items()):
            if env_key.startswith("VAR_"):
                var_to_replace = env_key.replace("VAR_", "")
                query_string = query_string.replace(var_to_replace, env_value)

        if self.report_on_all_dates:
            where_index = query_string.find('where')
            query_string = query_string[0:where_index]
        else:
            query_string = query_string.replace('{AccountId}', self.account_id)
            query_string = query_string.replace('{RegionId}', self.deployment_region)
            
            
            # job_date in 2020-09-23 format and try it all
            query_string = query_string.replace('{UtcToday}',
                                                datetime.utcnow().strftime("%Y-%m-%d"))
            query_string = query_string.replace('{LocalToday}',
                                                datetime.now().strftime("%Y-%m-%d"))
                                                
            query_string = query_string.replace('{UtcYear}',
                                                datetime.utcnow().strftime("%Y"))                                                    

            query_string = query_string.replace('{UtcMonth}',
                                                datetime.utcnow().strftime("%m"))
                                                
            query_string = query_string.replace('{UtcDay}',
                                                datetime.utcnow().strftime("%d"))  
                                                
            query_string = query_string.replace('{LocalYear}',
                                                datetime.utcnow().strftime("%Y"))                                                    

            query_string = query_string.replace('{LocalMonth}',
                                                datetime.utcnow().strftime("%m"))
                                                
            query_string = query_string.replace('{LocalDay}',
                                                datetime.utcnow().strftime("%d"))  
                                                    
        return query_string

    def move_results_file(self, src_bucket, src_key, dest_bucket, dest_key):
        """
        Helper function to move athena results file to the selected S3 URI
        """
        s3_resource = boto3.resource('s3')
        try:
            info(
                f"srcBucket : {src_bucket}, srcKey : {src_key}, destBucket : {dest_bucket}, destKey : {dest_key} ")
            # Create a source Dictionary That Specifies Bucket Name and Key Name of the Object to Be Copied
            copy_source = {
                'Bucket': src_bucket,
                'Key': src_key
            }

            # Copying the Object to the Target Directory
            s3_resource.Bucket(dest_bucket).copy(copy_source, dest_key)
            try:
                s3_resource.ObjectAcl(dest_bucket, dest_key).put(ACL='bucket-owner-full-control')
            except Exception as exc:                
                error(f"[IGNORED] Exception while trying ObjectAcl: {str(exc)}")
                
            # To Delete the File After Copying It to the Target Directory
            s3_resource.Object(src_bucket, src_key).delete()
        except Exception as exc:
            error(f"Exception: {str(exc)}")
            raise

    # Copy csv query results from s3 to local path
    def wait_query_execution_and_copy_results(self, context, query_list_to_process):
        """
        Helper function to wait for the Athena queries to complete and copy the results
        """
        s3_resource = boto3.resource('s3')
        info(f"Query Execution started, remaining : {context.get_remaining_time_in_millis()}")
        athena_client = boto3.client('athena')
        query_id_list = [x['queryId'] for x in query_list_to_process if
                         x['State'] not in ('SUCCEEDED', 'FAILED')]
        date_in_utc = datetime.utcnow().strftime("%m_%d_%Y_%H_%M_%S")
        time_remaining = context.get_remaining_time_in_millis()
        while len(query_id_list) > 0 and context.get_remaining_time_in_millis() > \
                self.athena_query_expiration_timeout_in_milli_sec:

            info(f"queryIdList : {query_id_list}")
            resp = athena_client.batch_get_query_execution(QueryExecutionIds=query_id_list)
            query_exec_response_list = resp['QueryExecutions']
            for query_exec_response in query_exec_response_list:
                if 'Status' in query_exec_response and 'State' in query_exec_response['Status']:
                    state = query_exec_response['Status']['State']
                    # https://stackoverflow.com/questions/598398/searching-a-list-of-objects-in-python
                    item_to_remove = [x for x in query_list_to_process
                                      if x['queryId'] == query_exec_response['QueryExecutionId']]
                    if state == 'SUCCEEDED':
                        s3_path = query_exec_response['ResultConfiguration']['OutputLocation']
                        try:

                            local_file_name = item_to_remove[0]['Name'] + '.csv'
                            # https://stackoverflow.com/questions/42641315/s3-urls-get-bucket-name-and-path
                            s3_path_obj = urlparse(s3_path)
                            info(f"s3PathObj: {s3_path_obj}")
                            s3_key = s3_path_obj.path.lstrip('/')

                            info(
                                f"Downloading query result from : {s3_key} in Bucket : {self.s3_observer_log_bucket} to {local_file_name}")
                            os.chdir(self.lambda_temp_path)
                            s3_resource.Bucket(self.s3_observer_log_bucket).download_file(s3_key,
                                                                                          local_file_name)

                            self.move_results_file(self.s3_observer_log_bucket, s3_key,
                                                   self.s3_email_report_output_bucket,
                                                   self.email_report_output_s3_key + self.account_id + '_' + date_in_utc + '_' + local_file_name)

                            info(
                                'Moved to s3:' + self.s3_email_report_output_bucket + "/" + self.email_report_output_s3_key + date_in_utc + '_' + local_file_name)

                        except botocore.exceptions.ClientError as e:
                            error(e)
                            raise
                    if state in ('FAILED', 'SUCCEEDED'):
                        item_to_remove[0]['State'] = state
                        info(
                            f"Id: {query_exec_response['QueryExecutionId']}.queryExecResponse: {query_exec_response} at {state}")

                    info("Sleeping for 1 Second")
                    time.sleep(1)
                    time_remaining = context.get_remaining_time_in_millis()
            query_id_list = [x['queryId'] for x in query_list_to_process if
                             (x['State'] != 'SUCCEEDED' and x['State'] != 'FAILED')]

        info(f"waitQueryExecutionAndCopyResults finished, timeRemaining : {time_remaining}")

    # Send Backup report via SES
    def send_report(self, query_list):
        """
        Helper function to send the report with the query result attachments
        """
        ses_client = boto3.client('ses')
        info(
            f"Starting sendReport using files from : {self.lambda_temp_path}, queryList : {query_list} ")

        # Create a multipart/mixed parent container.
        msg = MIMEMultipart('mixed')
        # Add subject, from and to lines.
        msg['Subject'] = self.replace_variables(self.ses_email_subject) 
        msg['From'] = self.ses_email_sender
        msg['To'] = self.ses_email_recipient_list
        os.chdir(self.lambda_temp_path)
        fileAttachementDetails = '<ul>'
        # for i in range(len(queryList)):
        for query_item in [x for x in query_list if x['State'] == 'SUCCEEDED']:
            fileNameToAttach = query_item['Name'] + '.csv'
            info(f"Processing attachment file. :{fileNameToAttach}")
            fileAttachementDetails = fileAttachementDetails + '<li>' + fileNameToAttach + '</li>'
            # Define the attachment part and encode it using MIMEApplication.
            att = MIMEApplication(open(fileNameToAttach, 'rb').read())
            # Add a header to tell the email client to treat this part as an attachment,
            # and to give the attachment a name.
            att.add_header('Content-Disposition', 'attachment', filename=fileNameToAttach)
            # Add the attachment to the parent container.
            msg.attach(att)
        fileAttachementDetails = fileAttachementDetails + '</ul>'
        self.ses_email_body_text = self.ses_email_body_text.replace('{AttachmentDetails}',fileAttachementDetails)
        self.ses_email_body_text = self.replace_variables(self.ses_email_body_text)
        msg.attach(MIMEText(self.ses_email_body_text,'html'))
        info(f"Sending messages to : {self.ses_email_recipient_list}")

        try:
            # Provide the contents of the email.
            response = ses_client.send_raw_email(
                RawMessage={
                    'Data': msg.as_string()
                }
            )
            response_message = "Email sent! Message ID:" + response['MessageId']
        # Display an error if something goes wrong.
        except ClientError as e:
            error(e.response['Error']['Message'])
            response_message = "Email sent failed, Error :" + e.response['Error']['Message']

        return response_message

    def get_query_list(self):
        """
        Helper function to get the list of queries to be executed
        """
        q_str_list = []
        for envKey, envValue in os.environ.items(): 
            if envKey.startswith('QUERY_'):
                name_from_env=envKey.replace('QUERY_','')
                query_from_env = envValue
                info(f'Processing : {query_from_env}')
                q_string = self.replace_variables(query_from_env)
                if q_string:
                    q_str_list.append(
                        {'Name': name_from_env, 'QueryString': q_string, 'State': 'PREPARED'})

        return q_str_list

    def verify_email_address(self, email_address):
        """
        Helper function to verify an email address in SES
        """
        info(f"verifyEmailAddress with : {email_address}")
        ses_client = boto3.client('ses')
        response = ses_client.get_identity_verification_attributes(Identities=[email_address])

        '''
        {
            'VerificationAttributes': {
                'string': {
                    'VerificationStatus': 'Pending'|'Success'|'Failed'|'TemporaryFailure'|'NotStarted',
                    'VerificationToken': 'string'
                }
            }
        }
        '''

        response_status = 'NotStarted'
        info(f"get_identity_verification_attributes : {response}")
        if email_address in response['VerificationAttributes']:
            response_status = response['VerificationAttributes'][email_address][
                'VerificationStatus']
            info(f"Email address : {email_address} in : {response_status} State.")

        if response_status != 'Success':
            response = ses_client.verify_email_identity(EmailAddress=email_address)
            info(
                f"Sent Email address : {email_address} verification with Status : {response_status}, Response : {response}")

    def handle_report_process_trigger(self, context):
        info('Starting handle_report_process_trigger')
        try:

            q_str_list = self.get_query_list()

            self.query_athena_database(q_str_list)
            self.wait_query_execution_and_copy_results(context, q_str_list)

            if self.ses_email_recipient_list:
                for recipient in self.ses_email_recipient_list.split(','):
                    self.verify_email_address(recipient)

                if not self.ses_email_sender:
                    self.ses_email_sender = self.ses_email_recipient_list.split(',')[0]

                info(
                    f"recipientList : {self.ses_email_recipient_list}, sender : {self.ses_email_sender}")
                self.send_report(q_str_list)
            else:
                info("SESEmailReportRecipientList is empty. Email will not be sent.")
        except Exception as exc:
            error(f"Exception: {str(exc)}")

    def __write_object_to_s3(self, s3_log_bucket, s3_log_location, json_content, write_source):
        """
        This method is responsible for writing the job info (jsonContent) to the
        folder (destFolder) under the key (jobId)
        """
        try:
            s3_client = boto3.client('s3')
            info(f"Logging to : {s3_log_bucket} at {s3_log_location}")
            s3_client.put_object(Body=json.dumps(json_content,
                                                 default=str, separators=(',', ':')),
                                 Bucket=s3_log_bucket,
                                 Key=s3_log_location, ACL='bucket-owner-full-control',
                                 Tagging='Source=BackupManager-' + write_source)
        except Exception:
            var = traceback.format_exc()
            error(f"Error {var} processing __write_object_to_s3")
            raise


    def process_backup_job_list(self,nextToken, over_write_files_in_s3):
        backup_client = boto3.client('backup')
        job_event_type = 'backup_job'

        request_params = {}
        request_params['NextToken'] = nextToken
        logger.info(f"backup_jobs_list with request_params : {request_params}")
        # boto3 API  /services/backup.html#Backup.Client.list_backup_jobs
        if nextToken:
            backup_jobs_list = backup_client.list_backup_jobs(request_params)
        else:
            backup_jobs_list = backup_client.list_backup_jobs()
            
        # logger.info(f"backup_jobs_list {backup_jobs_list}")

        for backupJobInfo in backup_jobs_list['BackupJobs']:
            # Check if the entry is already existing in the bucket
            job_id = backupJobInfo['BackupJobId']
            job_date = self.get_formatted_job_date(backupJobInfo['CreationDate'])
            if not self.job_details_exist_in_s3(job_event_type, job_date,
                                                job_id) or over_write_files_in_s3:
                logger.info(
                    f"Backup Job {job_id} does NOT EXIST or is marked for overwrite")
                self.write_backup_event_data(backupJobInfo['BackupJobId'],
                                             job_event_type)
            else:
                logger.info(
                    f"Backup Job {job_id} EXIST and is NOT marked for overwrite")
        
        #Check if more details exist
        if "NextToken" in backup_jobs_list:
            nextToken = backup_jobs_list["NextToken"]
            info(f'nextToken : {nextToken} exist. Querying for next batch')
            self.process_backup_job_list(nextToken,over_write_files_in_s3)

    def process_restore_job_list(self,nextToken,over_write_files_in_s3):
        
        backup_client = boto3.client('backup')
        job_event_type = 'backup_job'

        request_params = {}
        if nextToken:
            request_params["NextToken"] = nextToken

        
        # boto3 API  /services/backup.html#Backup.Client.list_restore_jobs""
        restore_jobs_list = backup_client.list_restore_jobs(request_params)

        for restoreJobInfo in restore_jobs_list['RestoreJobs']:
            # Check if the entry is already existing in the bucket
            job_id = restoreJobInfo['RestoreJobId']
            job_date = self.get_formatted_job_date(restoreJobInfo['CreationDate'])
            if not self.job_details_exist_in_s3(job_event_type, job_date,
                                                job_id) or over_write_files_in_s3:
                logger.info(
                    f"Restore Job {job_id} does NOT EXIST or is marked for overwrite")
                self.write_restore_event_data(restoreJobInfo['RestoreJobId'],
                                              job_event_type)
            else:
                logger.info(f"Restore Job {job_id} EXIST and is NOT marked for overwrite")

        #Check if more details exist
        if "NextToken" in restore_jobs_list:
            nextToken = restore_jobs_list["NextToken"]
            info(f'nextToken : {nextToken} exist. Querying for next batch')
            self.process_restore_job_list(nextToken,over_write_files_in_s3)                
    
    def process_copy_job_list(self,nextToken,over_write_files_in_s3):
        
        backup_client = boto3.client('backup')
        job_event_type = 'backup_job'

        request_params = {}
        if nextToken:
            request_params["NextToken"] = nextToken
            
        # boto3 API  /services/backup.html#Backup.Client.list_copy_jobs
        copy_jobs_list = backup_client.list_copy_jobs(request_params)

        for copyJobInfo in copy_jobs_list['CopyJobs']:
            # Check if the entry is already existing in the bucket
            job_id = copyJobInfo['CopyJobId']
            job_date = self.get_formatted_job_date(copyJobInfo['CreationDate'])
            if not self.job_details_exist_in_s3(job_event_type, job_date,
                                                job_id) or over_write_files_in_s3:
                logger.info(f"Copy Job {job_id} does NOT EXIST or is marked for overwrite")
                self.write_copy_event_data(copyJobInfo['CopyJobId'],
                                           job_event_type)
            else:
                logger.info(f"Copy Job {job_id} EXIST and is NOT marked for overwrite")

        #Check if more details exist
        if "NextToken" in copy_jobs_list:
            nextToken = copy_jobs_list["NextToken"]
            info(f'nextToken : {nextToken} exist. Querying for next batch')
            self.process_copy_job_list(nextToken,over_write_files_in_s3)                
            
    def handle_refresh_job_logs(self, event):
        job_event_type = event.get('EventType').lower()
        over_write_files_in_s3 = event.get('OverWriteFilesInS3').lower() in ['true', '1', 't', 'y',
                                                                             'yes']
        logger.info(f"Incoming Event Type : {job_event_type}")
        backup_client = boto3.client('backup')

        try:
            if job_event_type == 'backup_job':
                self.process_backup_job_list(None,over_write_files_in_s3)
            elif job_event_type == 'restore_job':
                self.process_restore_job_list(None,over_write_files_in_s3)
            elif job_event_type == 'copy_job':
                self.process_copy_job_list(None,over_write_files_in_s3)

        except (ClientError, Exception):
            var = traceback.format_exc()
            logger.error(f"Error {var} n handle_refresh_job_logs")

    def handle_scheduled_event(self, event, context):
        resources = event.get('resources')
        if resources:
            for resource in resources:
                if self.config_data_refresh_schedule_name and resource.endswith(
                        self.config_data_refresh_schedule_name):
                    try:
                        self.extract_config_details()
                    except Exception as e:  # pylint: disable = W0703
                        logger.error(e)
                elif self.backup_report_schedule_name and resource.endswith(
                        self.backup_report_schedule_name):

                    try:
                        self.handle_report_process_trigger(context)
                    except Exception as e:  # pylint: disable = W0703
                        logger.error(e)
                else:
                    logger.error("Unknown Scheduled Event invocation ")

    def handle_aws_backup_event(self,job_event_type,job_id):

        if job_event_type == 'backup_job':
            self.write_backup_event_data(job_id, job_event_type)

        elif job_event_type == 'restore_job':
            self.write_restore_event_data(job_id, job_event_type)

        elif job_event_type == 'copy_job':
            self.write_copy_event_data(job_id, job_event_type)
        else:
            logger.error(f"unknown job_event_type : {job_event_type} for aws.backup")
            return

        logger.info(
            f"Processed Event Type : {job_event_type} , Job Id : {job_id}")

    def handle_observer_command(self, event):
        payload = event.get('detail')
        logger.info(f"Handling event for observer.command, payload :{payload}")
        job_type = payload['job_type']
        
        if job_type in ('copy_job'):
            job_id = payload['job_id']
            copy_info = payload['copy_info']
            tag_info = payload['tag_info']
            
            additional_info_list = []
            self.enrich_copy_info_data(copy_info, additional_info_list)
            try:
                info(f"copy_info {copy_info}")
                # Log the Combined Event
                copy_event_data = {'job_id': job_id, 'job_region': self.deployment_region,
                                   'job_type': job_type,
                                   'job_date': self.get_formatted_job_date(
                                       copy_info['CopyJob']['CreationDate']),
                                   'copy_info': copy_info,
                                   'additional_info' : additional_info_list,
                                   'tag_info': tag_info
                }
                self.send_event_to_global_event_bus(job_type, copy_event_data)
            except (ClientError, Exception):
                var = traceback.format_exc()
                logger.error(f"Error {var} processing handle_observer_command")        

            
    def handle_observer_event(self, event):
        payload = event.get('detail')
        logger.info(f"Handling event for observer.events, payload :{payload}")
        job_type = payload['job_type']

        if job_type in ('backup_job', 'restore_job', 'copy_job', 'config_job/latest'):
            job_id = payload['job_id']
            # Persist the data to the reporting bucket
            self.save_event_data_to_s3(job_type, job_id, payload)
        elif job_type == 'config_archive':
            # Clear the stale config files
            self.move_older_files_to_archive('config_job/latest', 'config_job/archive')


    def handle_backup_audit_manager_report_refresh(self):
        """
          Helper function to process existing reports from AWS Backup Audit Manager report locations
        """
        if self.backup_audit_manager_report_folder:
            self.handle_backup_audit_manager_report_refresh_for_folder(self.backup_audit_manager_report_folder)
    
    def handle_backup_audit_manager_report_refresh_for_folder(self,s3_folder_prefix):
        if self.backup_audit_manager_report_bucket:
            s3_client = boto3.client('s3')
            response = s3_client.list_objects_v2(Bucket=self.backup_audit_manager_report_bucket,
                                                 Delimiter='/',
                                                 Prefix=s3_folder_prefix)
            if 'Contents' in response:
                for key_info in response['Contents']:
                    backup_audit_manager_report_key = key_info['Key']
                    self.handle_backup_audit_manager_report(self.backup_audit_manager_report_bucket,backup_audit_manager_report_key)
            elif 'CommonPrefixes' in response and len(response['CommonPrefixes']) > 0:
                for commonPrefix in response['CommonPrefixes']:
                    self.handle_backup_audit_manager_report_refresh_for_folder(commonPrefix['Prefix'])
            else:
                logger.info(f'NO Report file exists in the location : {s3_folder_prefix} on {self.backup_audit_manager_report_bucket}')
            
    def handle_backup_audit_manager_report(self, s3_bucket,s3_object_key):
        """
        This method is responsible for handling the AWS Backup Audit Manager report at {s3_object_key}
        in {s3_bucket}
        """    
        try:
            if not (s3_object_key.endswith('.json') or s3_object_key.endswith('.csv')):
                logger.error(f'Skipping handle_backup_audit_manager_report for unknown key : {s3_object_key}')
                return
            
            logger.info(f'Processing update for {s3_object_key} in {s3_bucket}')
            s3_client = boto3.client('s3')
            
            #Extract the key details on the file being processed from S3 key
            self.extract_report_details(s3_object_key)
            os.chdir(self.lambda_temp_path)
            report_details = self.extract_report_details(s3_object_key)
            report_file_full_path = self.lambda_temp_path + '/' + report_details["report_file_name"]        
            s3_client.download_file(s3_bucket, s3_object_key, report_file_full_path)
            file_size = (os.stat(report_file_full_path).st_size / 1024)
            logger.info(f'Downloaded report to File : {report_file_full_path} , Size : {file_size}')
            
            if s3_object_key.endswith('.json'):
                report_file_type="json"
                json_data = json.load(open(report_file_full_path, "r"))
                self.handle_backup_audit_manager_json_data(s3_object_key,json_data)
            elif s3_object_key.endswith('.csv'):
                report_file_type="csv"
                with open(report_file_full_path) as csv_file:
                    csv_reader = csv.reader(csv_file, delimiter=',')
                    self.handle_backup_audit_manager_csv_data(s3_object_key,csv_reader)

            if self.s3_observer_log_bucket:
                try:
                    report_details = self.extract_report_details(s3_object_key)
                    report_details["report_file_type"]=report_file_type
                    s3_post_fix_key = self.prepare_s3_postfix_from_report_details(report_details)
                    s3_observer_log_location = self.s3_observer_log_location + '/' + s3_post_fix_key                    
                    info(f"Persisting event data to : {self.s3_observer_log_bucket}")
                    self.save_report_file_to_s3(self.s3_observer_log_bucket,s3_observer_log_location,report_file_full_path,'BackupAuditManagerObserver')
                    
                except (ClientError, Exception):
                    var = traceback.format_exc()
                    logger.error(f"Error {var} processing s3 write for handle_backup_audit_manager_report")
                    
        except Exception as e:  # pylint: disable = W0703
            logger.error(e)    
    
    
    def extract_report_details(self,s3_object_key):
        s3_key_list = s3_object_key.lower().split('/')
        report_file_name = s3_key_list[-1]
        report_name = s3_key_list[-2]
        report_day = s3_key_list[-3]
        report_month = s3_key_list[-4]
        report_year = s3_key_list[-5]
        report_region = s3_key_list[-6]
        report_account_id = s3_key_list[-7]
        job_event_type = report_file_name.split('_report_')[0]
        report_type = job_event_type + '_report'  
        report_details  = {"report_file_name": report_file_name, 
                            "report_name": report_name, 
                            "report_day":report_day, 
                            "report_month" :report_month, 
                            "report_year" : report_year,
                            "report_region": report_region,
                            "report_account_id" :report_account_id,
                            "job_event_type":job_event_type,
                            "report_type": report_type
        }
        return report_details

    def handle_backup_audit_manager_csv_data(self,s3_object_key,csv_reader):
        idx = 0
        batchIdx=1
        reportItems = []
        headerRow = None
        report_details = self.extract_report_details(s3_object_key)        
        report_original_name = report_details["report_file_name"]
        report_details["report_file_type"]="csv"        
        for row in csv_reader:
            if not headerRow:
                headerRow = row
            reportItems.append(row)
            idx += 1
            if idx > BACKUP_AUDIT_MANAGER_REPORT_MAX_ITEMS:
                #Send the contents over to reporter. Currently empty records will also be sent
                report_details["report_file_name"] = report_original_name.replace('.csv',f'_{batchIdx}.csv')
                self.send_report_batch_to_aggregator(report_details,reportItems )
                reportItems = []
                reportItems.append(headerRow)
                idx = 0
                batchIdx= batchIdx + 1


        if reportItems and len(reportItems) > 0:
            report_details["report_file_name"] = report_original_name.replace('.csv',f'_{batchIdx}.csv')
            self.send_report_batch_to_aggregator(report_details,reportItems )    
            
    def handle_backup_audit_manager_json_data(self,s3_object_key,json_data):
        idx = 0
        batchIdx=1
        reportItems = []
        report_details = self.extract_report_details(s3_object_key)
        report_details["report_file_type"]="json"
        report_original_name = report_details["report_file_name"]
        job_event_type = report_details["job_event_type"]
        if self.backup_audit_manager_reportitems_key in json_data and len(json_data[self.backup_audit_manager_reportitems_key]) > 0:
            for reportItem in json_data[self.backup_audit_manager_reportitems_key]:
                if job_event_type in ['backup_job', 'restore_job','copy_job']:
                    job_id = ''
                    if job_event_type == 'backup_job':
                        job_id = reportItem['backupJobId']
                    elif job_event_type == 'restore_job':
                        job_id = reportItem['restoreJobId']
                    elif job_event_type == 'copy_job':
                        job_id = reportItem['copyJobId']
                    try:
                        #Handle the unpacking of the report data
                        self.handle_aws_backup_event(job_event_type,job_id)
                    except Exception as e:
                        logger.error(f"Error {e} processing handle_aws_backup_event")            
                        
                reportItems.append(reportItem)
                idx = idx + 1
                if idx > BACKUP_AUDIT_MANAGER_REPORT_MAX_ITEMS:
                    #Send the contents over to reporter. Currently empty records will also be sent
                    report_details["report_file_name"] = report_original_name.replace('.json',f'_{batchIdx}.json')
                    self.send_report_batch_to_aggregator(report_details,reportItems )
                    reportItems = []
                    idx = 0
                    batchIdx= batchIdx + 1                
        else:
            logger.error(f'Data at {s3_object_key} does not contain the expected item : {self.backup_audit_manager_reportitems_key} or enough item(s). Skipping processing.')
        
        if reportItems and len(reportItems) > 0:
            report_details["report_file_name"] = report_original_name.replace('.json',f'_{batchIdx}.json')
            self.send_report_batch_to_aggregator(report_details,reportItems )       
    
    def send_report_batch_to_aggregator(self, report_details,reportItems ):
        logger.info(f'send_report_batch_to_aggregator : report_details : {report_details}, reportItems length: {len(reportItems)}')
        json_content = { 
                            'report_file_name': report_details["report_file_name"], 
                            'report_name': report_details["report_name"], 
                            'report_day': report_details["report_day"], 
                            'report_month': report_details["report_month"], 
                            'report_year': report_details["report_year"], 
                            'report_region': report_details["report_region"], 
                            'report_account_id': report_details["report_account_id"], 
                            'report_type': report_details["report_type"],
                            'report_file_type':report_details["report_file_type"],
                            self.backup_audit_manager_reportitems_key : reportItems 
                        }
        s3_post_fix_key = self.prepare_s3_postfix_from_report_details(report_details)                
        self.send_report_data_to_aggregator(s3_post_fix_key,json_content)

    def prepare_s3_postfix_from_report_details(self, report_details):
        return report_details["report_type"] + '/' + report_details["report_file_type"] + '/' + report_details["report_account_id"] + '/' + report_details["report_region"] + '/' + report_details["report_year"] + '/' + report_details["report_month"] + '/' + report_details["report_day"] + '/' + report_details["report_name"] + '/' + report_details["report_file_name"]

    def save_report_file_to_s3(self, s3_bucket_name,s3_report_location,report_file_name,writeSource):
        """
        This method is responsible for uploading the report file (report_file_name) to the
        folder (s3_report_location) under the bucket (s3_bucket_name)
        """
        try:
            s3_client = boto3.client('s3')
            logger.info(f"uploading payload to : {s3_bucket_name} at {s3_report_location}")
            extraArgsForUpload = {'ACL':'bucket-owner-full-control', 'Tagging':'Source=' + writeSource}
            s3_client.upload_file(Filename=report_file_name, Bucket=s3_bucket_name, Key=s3_report_location,ExtraArgs=extraArgsForUpload)
            
        except (ClientError, Exception):
            var = traceback.format_exc()
            logger.error(f"Error {var} processing save_report_data_to_s3")

    
    def save_report_data_to_s3(self, s3_bucket_name,s3_report_location,json_payload):
        """
        This method is responsible for writing the report info (json_payload) to the
        folder (s3_report_location) under the bucket (s3_bucket_name)
        """
        try:
            s3_client = boto3.client('s3')
            logger.info(f"Writing JSON payload to : {s3_bucket_name} at {s3_report_location}")
            s3_client.put_object(Body=json_payload,Bucket=s3_bucket_name,
                                 Key=s3_report_location, ACL='bucket-owner-full-control',
                                 Tagging='Source=BackupAuditManagerObserver')                                  
        except (ClientError, Exception):
            var = traceback.format_exc()
            logger.error(f"Error {var} processing save_report_data_to_s3")
            
    def send_report_data_to_aggregator(self, s3_post_fix_key, json_content):
        """
        Utility function to send the json payload to the aggregator location
        """    
        events_client = boto3.client('events')
        try:
            s3_observer_log_location = self.s3_observer_log_location + '/' + s3_post_fix_key
            
            #Save the contents to the S3 Bucket
            json_payload = json.dumps(json_content, default=str, separators=(',', ':'))
            self.save_report_data_to_s3(self.s3_observer_log_bucket,s3_observer_log_location,json_payload)
            
            json_payload = json.dumps(json_content, default=str, separators=(',', ':'))
            if self.global_event_bus_arn:
                #Send this data to the aggregation location as well        
                logger.info(f"Sending event data to : {self.global_event_bus_arn} under type : {json_content['report_type']}")
                eb_entries = [
                    {
                        'Time': datetime.now(timezone.utc),
                        'Source': 'backupauditmanager.events',
                        'DetailType': json_content['report_type'],
                        'Detail': json_payload,
                        'EventBusName': self.global_event_bus_arn
                    }]
                response = events_client.put_events(Entries=eb_entries)
                logger.info(f"put_events response : {response}")
        except (ClientError, Exception):
            var = traceback.format_exc()
            logger.error(f"Error {var} processing put_events")    
            
    def handle_backup_audit_manager_event(self, event):
    
        payload = event.get('detail')
        logger.info(f"Handling event for backupauditmanager.events, payload :{payload}")
    
        #Get the attributes from the message and re-create the URL
        report_file_name = payload['report_file_name']
        report_name = payload['report_name']
        report_day = payload['report_day']
        report_month = payload['report_month']
        report_year = payload['report_year']
        report_region = payload['report_region']
        report_account_id = payload['report_account_id']
        report_type = payload['report_type']
        report_file_type= payload['report_file_type']
    
        report_details  = {"report_file_name": report_file_name, 
                            "report_name": report_name, 
                            "report_day":report_day, 
                            "report_month" :report_month, 
                            "report_year" : report_year,
                            "report_region": report_region,
                            "report_account_id" :report_account_id,
                            "report_type": report_type,
                            "report_file_type": report_file_type
        }
        s3_post_fix_key = self.prepare_s3_postfix_from_report_details(report_details)                            
        s3_observer_log_location = self.s3_observer_log_location + '/' + s3_post_fix_key
        logger.info(f'Saving report to {s3_observer_log_location} for Report :{report_type}')
        if report_file_name.endswith('.json'):
            self.save_backup_audit_manager_json_report(self.s3_observer_log_bucket,s3_observer_log_location,payload)
        elif report_file_name.endswith('.csv'):
            self.save_backup_audit_manager_csv_report(self.s3_observer_log_bucket,s3_observer_log_location,report_file_name,payload)
            
    def save_backup_audit_manager_csv_report(self,s3_observer_log_bucket,s3_observer_log_location,report_file_name,json_data):
        #Convert JSON Data to CSV Data
        temp_report_file_name = self.lambda_temp_path + '/' + report_file_name
        data_file = open(temp_report_file_name, 'w') 
        csv_writer = csv.writer(data_file,quoting=csv.QUOTE_ALL)
        if self.backup_audit_manager_reportitems_key in json_data:
            for reportItem in json_data[self.backup_audit_manager_reportitems_key]:
                csv_writer.writerow(reportItem)
        data_file.close()
        self.save_report_file_to_s3(s3_observer_log_bucket,s3_observer_log_location,temp_report_file_name,'BackupAuditManagerReporter')    
        
    def save_backup_audit_manager_json_report(self,s3_observer_log_bucket,s3_observer_log_location,payload):
        json_payload = json.dumps(payload, default=str, separators=(',', ':'))
        self.save_report_data_to_s3(s3_observer_log_bucket,s3_observer_log_location,json_payload)            
        
        
